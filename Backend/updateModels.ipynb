{"cells":[{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import boto3\n","from smart_open import smart_open\n","import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Data From Bucket"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["#AWS Credentials\n","aws_access = 'AKIA3OQDEGY5IF2353GU'\n","aws_secret_access = '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'\n","\n","#Creating S3 client\n","s3 = boto3.client('s3', aws_access_key_id = aws_access, aws_secret_access_key = aws_secret_access)\n","\n","#Bucket Directory\n","bucket = 'electricitydemandforecasting'\n","file = 'Data/data_for_analysis/actuals_1.csv'\n","file_2 = 'Data/data_for_analysis/actuals_2.csv'\n","file_3 = 'Data/Forecast_Data/forecasts.csv'\n","file_4 = 'Data/Forecast_Data/Forecasts_part2.csv'\n","\n","file_path = 's3://{}:{}@{}/{}'.format(aws_access, aws_secret_access, bucket, file)\n","file_path_2 = 's3://{}:{}@{}/{}'.format(aws_access, aws_secret_access, bucket, file_2)\n","\n","file_test = 's3://{}:{}@{}/{}'.format(aws_access, aws_secret_access, bucket, file_3)\n","file_test_2 = 's3://{}:{}@{}/{}'.format(aws_access, aws_secret_access, bucket, file_4)\n","\n","df = pd.read_csv(smart_open(file_path))\n","df2 = pd.read_csv(smart_open(file_path_2))\n","\n","#Create Whole Dataset\n","df_train = pd.concat([df, df2], axis = 0)\n","df_train['Time'] = pd.to_datetime(df_train['Time'])\n","\n","\n","df_test_1 = pd.read_csv(smart_open(file_test))\n","df_test_2 = pd.read_csv(smart_open(file_test_2))\n","df_test = pd.concat([df_test_1, df_test_2], axis = 0)\n","df_test['Time'] = pd.to_datetime(df_test['Time'])"]},{"cell_type":"markdown","metadata":{},"source":["### combine dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def combine_rows_with_different_attribute_names(df, df_test):\n","    combined_data_list = []\n","\n","    # start at 0, stop 168 + 1 rows before end, jump by 24 hours / 1 day.\n","    for i in range(0, len(df_test) - (168 + 1), 24):\n","        combined_data = {}\n","        \n","        if i + 168 + 24 >= (len(df_test)):\n","            break\n","        \n","        for j in range(168):\n","            row = df.iloc[i + j]\n","\n","            # Iterate over columns and update column names\n","            for col, val in row.items():\n","                combined_data[f\"{col}_date{j+1}\"] = val\n","        \n","        # Append the combined data dictionary to the list\n","        \n","        for j in range(48):\n","            # if i + j + 168 >= (len(df_test) - 1):\n","            #     break\n","            row = df_test.iloc[i + j + 168]\n","            \n","            for col, val in row.items():\n","                combined_data[f\"{col}_test_date{j+1}\"] = val\n","            \n","            \n","        combined_data_list.append(combined_data)\n","\n","    # Convert the list of dictionaries into a DataFrame\n","    combined_df = pd.DataFrame(combined_data_list)\n","\n","    return combined_df\n","\n","# Example usage:\n","# Assuming you have a DataFrame named df_train\n","formatted_data = combine_rows_with_different_attribute_names(df_train, df_test)"]},{"cell_type":"markdown","metadata":{},"source":["### the actual values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def actual(df_train):\n","    # Initializing the list to store the combined load data for every two overlapping days\n","    overlapping_grouped_data = []\n","\n","    # Iterate over the dataframe in steps of 24 hours to create overlapping groups of 48 hours (2 days)\n","    for i in range(0, len(df_train) - 48 + 1, 24):\n","        two_day_data = df_train.iloc[i:i+48]['Load (kW)'].values\n","        overlapping_grouped_data.append(two_day_data)\n","\n","    # Convert the grouped data into a dataframe\n","    combined_load_df = pd.DataFrame(overlapping_grouped_data)\n","\n","    # Drop the first 7 rows from overlapping_combined_load_df\n","    combined_load_df = combined_load_df.iloc[7:]\n","\n","    return combined_load_df\n","\n","actual_data = actual(df_train)"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_metrics(predictions, actuals):\n","    mae_per_sublist = []\n","    rmse_per_sublist = []\n","    mape_per_sublist = []\n","    r2_per_sublist = []\n","\n","    for i in range(len(predictions)):\n","        if len(predictions[i]) == 0 or len(actuals[i]) == 0:\n","            break\n","\n","        predicted_load = np.array(predictions[i])\n","        actual_load = np.array(actuals[i])\n","\n","        # Calculate the metrics for the current sublist\n","        mae = mean_absolute_error(actual_load, predicted_load)\n","        mse = mean_squared_error(actual_load, predicted_load)\n","        rmse = np.sqrt(mse)\n","        mape = np.mean(np.abs((actual_load - predicted_load) / actual_load)) * 100\n","        r_squared = r2_score(actual_load, predicted_load)\n","\n","        # Calculate the MAE for the current sublist\n","        mae = mean_absolute_error(actual_load, predicted_load)\n","        \n","        # Calculate the RMSE for the current sublist\n","        mse = mean_squared_error(actual_load, predicted_load)\n","        rmse = np.sqrt(mse)\n","        \n","        # Calculate the MAPE for the current sublist\n","        mape = np.mean(np.abs((actual_load - predicted_load) / actual_load)) * 100\n","        \n","        # Calculate the R-squared (R²) for the current sublist\n","        r_squared = r2_score(actual_load, predicted_load)\n","        \n","        # Append Metrics\n","        mae_per_sublist.append(mae)\n","        rmse_per_sublist.append(rmse)\n","        mape_per_sublist.append(mape)\n","        r2_per_sublist.append(r_squared)\n","\n","        # Check if there are sublists with non-empty data\n","    if len(mae_per_sublist) > 0:\n","        # Calculate the overall metrics by taking the mean of all sublists' values\n","        overall_mae = np.mean(mae_per_sublist)\n","        overall_rmse = np.mean(rmse_per_sublist)\n","        overall_mape = np.mean(mape_per_sublist)\n","        overall_r2 = np.mean(r2_per_sublist)\n","\n","        print(\"Overall Metrics:\")\n","        print(f\"Overall MAE: {overall_mae}\")\n","        print(f\"Overall RMSE: {overall_rmse}\")\n","        print(f\"Overall MAPE: {overall_mape}\")\n","        print(f\"Overall R-squared (R²): {overall_r2}\")\n","    else:\n","        print(\"No valid data to calculate metrics.\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Graph for the last two values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def graph(formatted_data, predictions, actuals):\n","    two_values = pd.DataFrame({\n","        \"Time\" : formatted_data.filter(like=\"Time\").iloc[-1].tail(48).values,\n","        \"predictions\" : predictions[-1],\n","        \"actuals\" : actuals[-1]\n","    })\n","    # Adjusting the plot settings\n","    fig, ax = plt.subplots(figsize=(15, 7))\n","\n","    # Plotting 'predictions' and 'actuals' on the same axis\n","    ax.plot(two_values['Time'], two_values['predictions'], color='b', label='Predictions')\n","    ax.plot(two_values['Time'], two_values['actuals'], color='r', label='Actuals')\n","    ax.set_xlabel('Time')\n","    ax.set_ylabel('Values')\n","    ax.tick_params('y')\n","\n","    # Displaying all x-axis labels\n","    ax.set_xticks(two_values['Time'])\n","    ax.set_xticklabels(two_values['Time'].dt.strftime('%Y-%m-%d %H:%M:%S'), rotation=45, fontsize=10)\n","\n","    # Removing scientific notation\n","    ax.ticklabel_format(style='plain', axis='y')\n","\n","    # Setting the title, legend, and showing the plot\n","    ax.legend(loc='upper left')\n","    plt.title('Predictions vs. Actuals')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Naive model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def naive(formatted_data,actual_data):\n","    \"\"\"\n","    :Input: \n","        :data: The entire dataset of batches.\n","        \n","    For this algorithm, go over each batch, create a list of predctions,\n","    compare these predictions to the predictions to actuals, which can be done through the \n","    next row which introduce the data for the next two days.\n","    \"\"\"\n","\n","    predictions = []\n","    actuals = []\n","    for index, row in formatted_data.iterrows():\n","        predictions.append([])\n","        actuals.append([])\n","        for i in range(121, 169):\n","            column_load = f\"Load (kW)_date{i}\"\n","            column_time = f\"Time_date{i}\"\n","            \n","            # predictions[index] = [load, time of load used to make prediction]\n","            # The load above is for the time to the right plus two days.\n","            \n","            # predictions[index].append([row[column_load], row[column_time]])\n","            \n","            # Just Predicted Load\n","            predictions[index].append(row[column_load]) \n","\n","            \n","    actuals = actual_data.values\n","\n","    return predictions, actuals\n","\n","    \n","n_predictions, n_actuals = naive(formatted_data,actual_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Naive:\")\n","calculate_metrics(n_predictions, n_actuals)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph(formatted_data, n_predictions, n_actuals)"]},{"cell_type":"markdown","metadata":{},"source":["### Random walk model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def random_walk_forecast(formatted_data,actual_data):\n","    \"\"\"\n","    Generate random walk forecasts.\n","    \"\"\"\n","    predictions = []\n","    actuals = []\n","    \n","    for index, row in formatted_data.iterrows():\n","        last_observed_value = row[\"Load (kW)_date168\"]\n","        batch_predictions = [last_observed_value + np.random.normal(0, 1) for _ in range(48)]\n","        predictions.append(batch_predictions)\n","        \n","        # Extracting actuals with a two-row offset\n","        \n","    actuals = actual_data.values\n","            \n","\n","    return predictions, actuals\n","\n","\n","rw_predictions, rw_actuals = random_walk_forecast(formatted_data,actual_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Random walk:\")\n","calculate_metrics(rw_predictions, rw_actuals)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph(formatted_data, rw_predictions, rw_actuals)"]},{"cell_type":"markdown","metadata":{},"source":["### Lightgbm model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lightgbm(formatted_data, actual_data):\n","    # Removing columns related to \"Time\" from formatted_data\n","    features_without_time = formatted_data.drop(columns=[col for col in formatted_data.columns if 'Time' in col])\n","    features =features_without_time\n","\n","    targets = actual_data\n","    # Splitting the data into training and validation sets (80% training, 20% validation)\n","    X_train, X_val, y_train_df, y_val_df = train_test_split(features, targets, test_size=0.2, random_state=50)\n","\n","    # Setting LightGBM parameters\n","    params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',\n","        'metric': 'rmse',\n","        'num_leaves': 31,\n","        'learning_rate': 0.05,\n","        'feature_fraction': 0.9\n","    }\n","\n","    num_round = 100\n","\n","    models = []\n","    pred = []\n","    for column in y_train_df.columns:\n","        y_train = y_train_df[column].values\n","        y_val = y_val_df[column].values\n","        \n","        # Creating a LightGBM dataset\n","        train_dataset = lgb.Dataset(X_train, label=y_train)\n","        val_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)\n","        \n","        # Training the model\n","        bst = lgb.train(params, train_dataset, num_round, valid_sets= val_dataset, callbacks = [lgb.early_stopping(stopping_rounds=10)])\n","        \n","        # Storing the trained model\n","        models.append(bst)\n","\n","        # Predicting on the validation set\n","        y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n","        \n","        pred.append(y_pred)\n","\n","    # Transposing the list\n","    pred = list(map(list, zip(*pred)))\n","    y_val_df = y_val_df.values\n","\n","    return pred, y_val_df\n","\n","lightgbm_predictions, lightgbm_actuals = lightgbm(formatted_data, actual_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Lightgbm Model\")\n","calculate_metrics(lightgbm_predictions, lightgbm_actuals)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph(formatted_data, lightgbm_predictions, lightgbm_actuals)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
